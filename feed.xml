<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SIGNAL Lab</title>
    <description>Situated Grounding and Natural Language &lt;br/&gt; NLP @ CSU</description>
    <link>https://www.signallab.ai/https://www.signallab.ai//</link>
    <atom:link href="https://www.signallab.ai/https://www.signallab.ai//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 18 Jul 2024 04:05:42 +0000</pubDate>
    <lastBuildDate>Thu, 18 Jul 2024 04:05:42 +0000</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Best Paper Award at EDM 2024</title>
        <description>&lt;p&gt;Exciting news! Our paper, &lt;em&gt;Propositional Extraction from Natural Speech in Small Group Collaborative Tasks&lt;/em&gt;, was awarded Best Student Paper at the International Conference on Educational Data Mining 2024. This paper addresses the problem of extracting the semantics expressed by spoken utterances in a multiparty context. In spoken dialogue, the same semantic content can be expressed in many different ways. While it’s easy for humam to interpret the may ways of saying the same thing, it is challenging for computers for many reasons, such as filler words, disfluencies, and overlapping utterances. We adapted a cross-encoding method from coreference research in NLP to address this problem and perform significantly better than a vector-similarity approach, and achieve a strong baseline in this novel, challenging task. We also analyze the impact that transcription errors from automatic speech recognition have on performace, and find that with our cross-encoding approach, the impact of such errors can be substantially minimized.&lt;/p&gt;

&lt;p&gt;Congratulations to Videep, Abhijnan, Ibrahim, Avyakta, and Mariah!&lt;/p&gt;

&lt;p&gt;First author &lt;strong&gt;Videep Venkatesha&lt;/strong&gt; was in Atlanta this week to present the paper and accept the award. This paper was based on material funded by both the NSF iSAT institute and the DARPA FACT program, and is a collaboration with Brandeis University. The link to the proceedings can be found &lt;a href=&quot;https://educationaldatamining.org/edm2024/proceedings/2024.EDM-long-papers.14/2024.EDM-long-papers.14.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Citation: Venkatesha, V., Nath, A., Khebour, I., Chelle, A., Bradford, M., Tu, J., Pustejovsky, J., Blanchard, N., and Krishnaswamy, N. (2024). Propositional Extraction from Natural Speech in Small Group Collaborative Tasks. In &lt;em&gt;International Conference on Educational Data Mining (EDM)&lt;/em&gt;. International EDM Society.&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2024/07/17/best-paper-award-edm-2024.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Wed, 17 Jul 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//best-paper-award-edm-2024</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//best-paper-award-edm-2024</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>awards</category>
        
        
      </item>
    
      <item>
        <title>Congratulations to Nada Alalyani!</title>
        <description>&lt;p&gt;Congratulations to Dr. &lt;strong&gt;Nada Alalyani&lt;/strong&gt; for successfully defending her Ph.D. dissertation!&lt;/p&gt;

&lt;p&gt;In Nada’s dissertation, &lt;em&gt;Embodied Multimodal Referring Expressions Generation&lt;/em&gt;, she takes the Diana system which I worked on through my postdoc and uses it to train an LLM that can drive interactive virtual avatar behavior to refer to objects in context using mixed modalities. It’s also somewhat bittersweet for me because this represents probably the last research done with the Diana system from the CwC project.&lt;/p&gt;

&lt;p&gt;Congratulations to Nada!  I’ve been continually impressed by your ability to take challenging ideas and turn them into reality. Best of luck in your future endeavors!&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2024/06/25/congratulations-nada-alalyani.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Tue, 25 Jun 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//congratulations-nada-alalyani</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//congratulations-nada-alalyani</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>Here are three of us at LREC-COLING 2024</title>
        <description>&lt;p&gt;Here are three of us outside the Lingotto Conference Center in Turin, Italy, site of LREC-COLING 2024, held from May 20-25. From left to right: Shadi Manafi, Nikhil Krishnaswamy, Abhijnan Nath, and Ibrahim Khebour.&lt;/p&gt;

&lt;p&gt;We had one poster presentation and two talks, and LREC-COLING hired a (really quite good, though very loud) Queen cover band for the gala dinner.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/summer24/lrec-coling-2024.jpg?raw=true&quot; alt=&quot;LREC-COLING 2024&quot; title=&quot;LREC-COLING 2024&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 31 May 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//lrec-coling-2024</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//lrec-coling-2024</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>CS Department Scholarship Recipients</title>
        <description>&lt;p&gt;Three SIGNAL Lab students were awarded scholarships by the CSU CS department for meritorious achievements in AI/computer science.&lt;/p&gt;

&lt;p&gt;The recipients are: &lt;strong&gt;Abhijnan Nath&lt;/strong&gt;, winner of the Evolutionary Computation and Artificial Intelligence Graduate Fellowship; &lt;strong&gt;Sheikh Mannan&lt;/strong&gt;, winner of the Computer Science Graduate Fellowship; and &lt;strong&gt;Shadi Manafi&lt;/strong&gt;, winner of the Wim Böhm and Partners Fellowship.  Congratulations to Abhijnan, Mannan, and Shadi!&lt;/p&gt;
</description>
        <pubDate>Mon, 06 May 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//cs-department-scholarship-recipients</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//cs-department-scholarship-recipients</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>awards</category>
        
        
      </item>
    
      <item>
        <title>Congratulations to Hannah VanderHoeven!</title>
        <description>&lt;p&gt;Congratulations to &lt;strong&gt;Hannah VanderHoeven&lt;/strong&gt; on successfully defending her Master’s thesis!&lt;/p&gt;

&lt;p&gt;Hannah’s thesis is entitled &lt;em&gt;Robust Gesture Detection for Multimodal Problem Solving&lt;/em&gt;, and comprises a synthesis of a few papers that appeared (or will appear) at HCII over these couple years [&lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/HCII-2023-VanderHoeven.pdf&quot;&gt;1&lt;/a&gt;, &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/HCII-2024-VanderHoeven-Point.pdf&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/HCII-2024-VanderHoeven-Multimodal.pdf&quot;&gt;3&lt;/a&gt;], focusing on gesture detection techniques for use in collaborative agents for multimodal problem solving. This work originated in the iSAT project, which we recently completed a demo of to the NSF, and is being expanded for use in the FACT program.&lt;/p&gt;

&lt;p&gt;Congratulations to Hannah!  It’s been a pleasure working with you and I’m excited to see where this work goes next!&lt;/p&gt;

&lt;p&gt;I don’t have a picture of the defense, but here’s a picture from the poster session from the NSF iSAT site visit yesterday where similar work was presented. From left to right: Yifan Zhu (Brandeis University), Hannah, and SIGNAL Lab Ph.D. students Mariah Bradford and Ibrahim Khebour.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/spring24/nsf-site-visit-2024.jpg?raw=true&quot; alt=&quot;NSF Site Visit 2024&quot; title=&quot;NSF Site Visit 2024&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2024/05/01/congratulations-hannah-vanderhoeven.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Wed, 01 May 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//congratulations-hannah-vanderhoeven</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//congratulations-hannah-vanderhoeven</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>DARPA FACT AIE Award</title>
        <description>&lt;p&gt;I am beyond excited that we have been awarded a contract on the DARPA Friction and Accountability in Conversational Transactions (FACT) AI Exploration program! Our project, &lt;em&gt;TRACE: Transparency, Reflection, and Accountability in Conversational Exchanges&lt;/em&gt;, will address a lack of “friction” (deliberation and reflective reasoning) in LLMs that prevent them from being used reliably in mission-critical workflows.&lt;/p&gt;

&lt;p&gt;Put simply, LLMs are incentivized to accept the premises in whatever is input to them, even if false or faulty, and generate responses accordingly. This is due to an inability to track and assess the validity of beliefs held by their interlocutors. Theory of Mind (ToM) provides a way of measuring and tracking such beliefs, such as in the course of a shared collaborative task. Therefore we’re going to build ToM into LLMs.&lt;/p&gt;

&lt;p&gt;This project is a collaboration with Dr. James Pustejovsky at Brandeis University. Other CSU personnel include Dr. Nate Blanchard, Dr. Sarath Sreedharan, and Dr. Bruce Draper. I’m very excited to lead this excellent team!&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2024/02/23/darpa-fact-aie-award.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Fri, 23 Feb 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//darpa-fact-aie-award</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//darpa-fact-aie-award</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grants</category>
        
        
      </item>
    
      <item>
        <title>LREC-COLING Hat Trick and Other Stories</title>
        <description>&lt;p&gt;We are delighted to be 3 for 3 on submissions to LREC-COLING 2024, the first Joint International Conference on Computational Linguistics and Language Resources and Evaluation! The accepted papers are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Nath, A., Jamil, H., Ahmed, S. R., Baker, G., Ghosh, R., Martin, J. H., Blanchard, N., and Krishnaswamy, N. (2024). Multimodal Cross-Document Event Coreference Resolution Using Linear Semantic Transfer and Mixed-Modality Ensembles. In &lt;em&gt;Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING).&lt;/em&gt; ACL.
(In collaboration with the University of Colorado Boulder.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Khebour, I., Lai, K., Bradford, M., Zhu, Y., Brutti, R., Tam, C., Tu, J., Ibarra, B., Blanchard, N., Krishnaswamy, N., and Pustejovsky, J. (2024). Common Ground Tracking in Multimodal Dialogue. In &lt;em&gt;Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING).&lt;/em&gt; ACL.
(In collaboration with Brandeis University.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Manafi, S. and Krishnaswamy, N. (2024). Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets. In &lt;em&gt;Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING).&lt;/em&gt; ACL.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The conference will be held in Turin, Italy, in late May. Congratulations to SIGNAL Lab graduate students Abhijnan, Ibrahim, Mariah, and Shadi (and all affiliated students/external collaborators)!&lt;/p&gt;

&lt;p&gt;Additionally, Sheikh Mannan and Sadaf Ghaffari have a publication each at AAAI Spring Symposia, to be held at Stanford University in March:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ghaffari, S. and Krishnaswamy, N. (2024). Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics. In &lt;em&gt;AAAI Spring Symposium: Empowering Machine Learning and Large Language Models with Domain and Commonsense Knowledge (MAKE).&lt;/em&gt; AAAI.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mannan, S., Vimal, V. P., DiZio, P., and Krishnaswamy, N. (2024). Embodying Human-Like Modes of Balance Control Through Human-in-the-Loop Dyadic Learning. In &lt;em&gt;AAAI Spring Symposium: Symposium on Human-Like Learning (HLL).&lt;/em&gt; AAAI.
(In collaboration with Brandeis University.)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Last but not least, another hat trick (+1 bonus poster) at the International Conference on Human-Computer Interaction, to be held in Washington, DC, in June/July:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;VanderHoeven, H., Bradford, M., Jung, C., Khebour, I., Lai, K., Pustejovsky, J., Krishnaswamy, N., and Blanchard, N. (2024). Multimodal Design for Interactive Collaborative Problem Solving Support. In &lt;em&gt;International Conference on Human-Computer Interaction (HCII).&lt;/em&gt; Springer.
(In collaboration with Brandeis University.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Zhu, Y., VanderHoeven, H., Lai, K., Bradford, M., Tam, C., Khebour, I., Brutti, R., Krishnaswamy, N., and Pustejovsky, J. (2024). Modeling Theory of Mind in Multimodal HCI. In &lt;em&gt;International Conference on Human-Computer Interaction (HCII).&lt;/em&gt; Springer.
(In collaboration with Brandeis University.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;VanderHoeven, H., Blanchard, N., and Krishnaswamy, N. (2024). Point Target Detection for Multimodal Communication. In &lt;em&gt;International Conference on Human-Computer Interaction (HCII).&lt;/em&gt; Springer.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Congratulations to Hannah (x3), Mariah, and Ibrahim again!&lt;/p&gt;

&lt;p&gt;And the poster:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Seefried, E., Bradford, M., Aich, S., Siebert, C., Krishnaswamy, N., and Blanchard, N. (2024). Learning Foreign Language Vocabulary Through Task-Based Virtual Reality Immersion. In &lt;em&gt;International Conference on Human-Computer Interaction (HCII).&lt;/em&gt; Springer.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 20 Feb 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//coling-hat-trick-other-stories</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//coling-hat-trick-other-stories</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>RETTL Grant Awarded</title>
        <description>&lt;p&gt;Pleased to announce that our grant proposal to NSF’s RETTL program, &lt;em&gt;An AI Tutoring System for Pollinator Conservation Community Science Training&lt;/em&gt;, has been awarded, for $849,890!&lt;/p&gt;

&lt;p&gt;This project is led by my colleague Dr. Sarath Sreedharan and includes Dr. Nate Blanchard of CSU’s Vision Lab and Dr. Jill Zarestky in the CSU School of Education.  In this project we will develop AI-powered tools for citizen science, focusing on identification of different pollinator species.  Let’s get them bees!&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2023/09/11/rettl-grant-awarded.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Sep 2023 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//rettl-grant-awarded</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//rettl-grant-awarded</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grants</category>
        
        
      </item>
    
      <item>
        <title>Spring 2023 Publication Round Up</title>
        <description>&lt;p&gt;At the end of the semester, time to take stock of lab activity:&lt;/p&gt;

&lt;p&gt;Hannah VanderHoeven’s paper &lt;em&gt;Robust Motion Recognition using Gesture Phase Annotation&lt;/em&gt; was accepted for publication at HCII 2023 in Copenhagen, Denmark in July!&lt;/p&gt;

&lt;p&gt;Also at HCII will be &lt;em&gt;Intentional Microgesture Recognition for Extended Human-Computer Interaction&lt;/em&gt;, with Sheikh Mannan and Hannah as contributing authors!&lt;/p&gt;

&lt;p&gt;Mariah Bradford and Ibrahim Khebour’s paper &lt;em&gt;Automatic Detection of Collaborative States in Small Groups Using Multimodal Features&lt;/em&gt; was accepted for publication at AIEd 2023 in Tokyo, Japan, also in July!&lt;/p&gt;

&lt;p&gt;Mariah and Ibrahim also contributed to &lt;em&gt;How Good is Automatic Segmentation as a Multimodal Discourse Annotation Aid?&lt;/em&gt;, accepted for presentation at the Interoperable Semantic Annotation workshop at IWCS in June.&lt;/p&gt;

&lt;p&gt;Sadaf Ghaffari’s paper &lt;em&gt;Grounding and Distinguishing Conceptual Vocabulary Through Similarity Learning in Embodied Simulations&lt;/em&gt; was accepted for publication at the IWCS 2023 main conference in Nancy, France!&lt;/p&gt;

&lt;p&gt;And finally, Abhijnan Nath and Mannan’s paper &lt;em&gt;AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese&lt;/em&gt; was accepted for publication in Findings of ACL 2023, the companion volume to the ACL 2023 conference!  Abhijnan was also second author on &lt;em&gt;2*n is better than n^2: Decomposing Event Coreference Resolution into Two Tractable Problems&lt;/em&gt;, a collaboration with members of the BoulderNLP group, also accepted to Finding of ACL 2023.  ACL 2023 will be held in Toronto, ON, Canada, in July.&lt;/p&gt;

&lt;p&gt;Look out for these papers, and more to come soon.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;VanderHoeven, H., Blanchard, N., and Krishnaswamy, N. (2023). &lt;em&gt;Robust Motion Recognition using Gesture Phase Annotation.&lt;/em&gt; In &lt;em&gt;International Conference on Human-Computer Interaction (HCII)&lt;/em&gt;. Springer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Kandoi, C., Jung, C., Mannan, S., VanderHoeven, H., Meisman, Q., Krishnaswamy, N., and Blanchard, N. (2023). &lt;em&gt;Intentional Microgesture Recognition for Extended Human-Computer Interaction.&lt;/em&gt; In &lt;em&gt;International Conference on Human-Computer Interaction (HCII)&lt;/em&gt;. Springer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bradford, M., Khebour, I., Blanchard, N., and Krishnaswamy, N. (2023). &lt;em&gt;Automatic Detection of Collaborative States in Small Groups Using Multimodal Features.&lt;/em&gt; In &lt;em&gt;International Conference on Artificial Intelligence in Education (AIEd)&lt;/em&gt;. International AIEd Society.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Terpstra, C., Khebour, I., Bradford, M., Wisniewski, B., Krishnaswamy, N., and Blanchard, N. (2023). &lt;em&gt;How Good is Automatic Segmentation as a Multimodal Discourse Annotation Aid?&lt;/em&gt; In &lt;em&gt;International Workshop on Semantic Annotation (ISA)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ghaffari, S. and Krishnaswamy, N. (2023). &lt;em&gt;Grounding and Distinguishing Conceptual Vocabulary Through Similarity Learning in Embodied Simulations.&lt;/em&gt; In &lt;em&gt;International Conference on Computational Semantics (IWCS)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nath, A., Mannan, S., and Krishnaswamy, N. (2023). &lt;em&gt;AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese.&lt;/em&gt; In &lt;em&gt;Findings of the Association for Computational Linguistics (Findings of ACL)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ahmed, S. R., Nath, A., Martin, J. H., and Krishnaswamy, N. (2023). &lt;em&gt;2*n is better than n^2: Decomposing Event Coreference Resolution into Two Tractable Problems.&lt;/em&gt; In &lt;em&gt;Findings of the Association for Computational Linguistics (Findings of ACL)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 14 May 2023 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//spring-2023-publication-roundup</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//spring-2023-publication-roundup</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>Congratulations to Aniket Tomar!</title>
        <description>&lt;p&gt;Congratulations to &lt;strong&gt;Aniket Tomar&lt;/strong&gt; on successfully defending his Master’s thesis!&lt;/p&gt;

&lt;p&gt;Aniket’s thesis is entitled &lt;em&gt;Exploring Correspondences Between Gibsonian and Telic Affordances for Object Grasping using 3D Geometry&lt;/em&gt;. This is a really interesting work in that Aniket took what was initially a negative result and probed it enough to draw a robust conclusion about the representation of Gibsonian and telic affordaces in 3D data and the difficulty of embodied tasks for even specialized machine learning models. Part of this work appeared in our &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/AREA-2022.pdf&quot;&gt;AREA workshop paper&lt;/a&gt; at ESSLLI last summer.&lt;/p&gt;

&lt;p&gt;Congratulations to Aniket!  It’s been a pleasure working with you and I look forward to seeing what you do next!&lt;/p&gt;

&lt;p&gt;Here we are after the defense with Aniket’s committee: myself, Prof. Nathaniel Blanchard and Prof. Ben Clegg from Psychology.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/spring23/aniket-defense.jpg?raw=true&quot; alt=&quot;Aniket Tomar MS Thesis Defense&quot; title=&quot;Aniket Tomar MS Thesis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2023/03/06/congratulations-aniket-tomar.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Mon, 06 Mar 2023 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//congratulations-aniket-tomar</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//congratulations-aniket-tomar</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
  </channel>
</rss>
