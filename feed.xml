<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SIGNAL Lab</title>
    <description>Situated Grounding and Natural Language &lt;br/&gt; NLP @ CSU</description>
    <link>https://www.signallab.ai//</link>
    <atom:link href="https://www.signallab.ai//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Wed, 09 Dec 2020 03:44:56 +0000</pubDate>
    <lastBuildDate>Wed, 09 Dec 2020 03:44:56 +0000</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Best Demo at ICAT-EGVE 2020</title>
        <description>&lt;p&gt;Our demo, &lt;em&gt;Situational Awareness in Human Computer Interaction: Diana’s World&lt;/em&gt; has won the &lt;a href=&quot;https://icat-egve-2020.org/awards/&quot;&gt;best demo award&lt;/a&gt; at ICAT-EGVE (International Conference on Artificial Reality and Telexistence &amp;amp; Eurographics Symposium on Virtual Environments), an ACM conference!  Our paper is &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/ICAT-EGVE-2020.pdf&quot;&gt;here&lt;/a&gt; and our demo video can be viewed &lt;a href=&quot;https://www.youtube.com/watch?v=0b2_PWS_QZ4&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The demo was presented on December 4th at the virtual conference.&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2020/12/08/best-demo-icat-egve-2020.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Dec 2020 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//best-demo-icat-egve-2020</link>
        <guid isPermaLink="true">https://www.signallab.ai//best-demo-icat-egve-2020</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        
      </item>
    
      <item>
        <title>SIGNAL Lab Colloquium Video</title>
        <description>&lt;p&gt;Traditionally, computer science faculty at CSU give rapid-fire research presentations at the beginning of the fall semester to present themselves to students and discuss research opportunities.  Due to the Covid situation pushing everything online, the seminar format is being flipped this year, with prerecorded videos being offered followed by Q+A during the scheduled session time.  I took the opportunity to create a 5-minute video overview of some of the SIGNAL Lab’s major research thrusts, complete with some video demonstrations.&lt;/p&gt;

&lt;p&gt;You can watch the video &lt;a href=&quot;http://nkrishna.powweb.com/video/BMAC-short.mp4&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Thu, 27 Aug 2020 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//signal-lab-colloquium-video</link>
        <guid isPermaLink="true">https://www.signallab.ai//signal-lab-colloquium-video</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        
      </item>
    
      <item>
        <title>Neurosymbolic AI at ACS 2020</title>
        <description>&lt;p&gt;Today I presented a long paper, “Neurosymbolic AI for Situated Language Understanding” at the Advances in Cognitive Systems conference, held virtually and hosted by the Palo Alto Research Center.&lt;/p&gt;

&lt;p&gt;This was third of three papers submitted as a postdoc and presented as a professor, and I’m really very proud of this one, as it’s a detailed yet concise summary of effectively the last five years of work at Brandeis in developing situated grounding and embodied AI under the Communicating With Computers program, and nicely lays out most of my graduate and postdoctoral career.&lt;/p&gt;

&lt;p&gt;The work discussed in this paper forms the foundations of the work in the SIGNAL Lab, and I hope that neurosymbolic AI is just getting started, and provides a number of opportunities for groundbreaking research that are barely on the horizon for the AI and cognitive systems communities.  You can find the paper &lt;a href=&quot;assets/docs/pdfs/ACS-2020.pdf&quot;&gt;here&lt;/a&gt;, the slides &lt;a href=&quot;assets/docs/slides/ACS-2020.pdf&quot;&gt;here&lt;/a&gt;, and a video of the talk &lt;a href=&quot;https://www.youtube.com/watch?v=IwIvn64mT3U&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2020/08/10/neurosymbolic-ai-at-acs-2020.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Mon, 10 Aug 2020 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//neurosymbolic-ai-at-acs-2020</link>
        <guid isPermaLink="true">https://www.signallab.ai//neurosymbolic-ai-at-acs-2020</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        
      </item>
    
      <item>
        <title>Introducing SIGNAL Lab</title>
        <description>&lt;p&gt;I’m pleased to announce the formation of my lab here at Colorado State, the &lt;strong&gt;Si&lt;/strong&gt;tuated &lt;strong&gt;G&lt;/strong&gt;rounding and &lt;strong&gt;Na&lt;/strong&gt;tural &lt;strong&gt;L&lt;/strong&gt;anguage Lab.  As the primary NLP group in the CSU Computer Science department, we aim to produce cutting-edge AI research in language processing and understanding that’s grounded in linguistic knowledge and multimodality, as well as cognitive science, philosophy, psychology, and more.  We collaborate extensively with other CSU CS labs, including the &lt;a href=&quot;https://www.cs.colostate.edu/~draper/CwC.php&quot;&gt;CWC&lt;/a&gt; and &lt;a href=&quot;https://nuilab.org/Home&quot;&gt;NUI&lt;/a&gt; Labs, and with groups at Brandeis University, Tufts University, and others.  Broadly, we study language and cognition using machine learning, logical architectures, and simulation methods. Much of our work involves intelligent, interactive, multimodal agents.&lt;/p&gt;

&lt;p&gt;We are currently seeking new researchers and collaborators, including students interested in language, AI, and machine learning. Prior experience or interest in graphics or software engineering is a plus, and women and underrepresented minorities are particularly welcome. Interested in working with us? Please &lt;a href=&quot;contact/&quot;&gt;get in touch!&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 01 Jul 2020 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//introducing-signal-lab</link>
        <guid isPermaLink="true">https://www.signallab.ai//introducing-signal-lab</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        
      </item>
    
  </channel>
</rss>
