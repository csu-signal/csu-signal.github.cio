<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SIGNAL Lab</title>
    <description>Situated Grounding and Natural Language &lt;br/&gt; NLP @ CSU</description>
    <link>https://www.signallab.ai/https://www.signallab.ai//</link>
    <atom:link href="https://www.signallab.ai/https://www.signallab.ai//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 07 Mar 2023 17:56:39 +0000</pubDate>
    <lastBuildDate>Tue, 07 Mar 2023 17:56:39 +0000</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Congratulations to Aniket Tomar!</title>
        <description>&lt;p&gt;Congratulations to &lt;strong&gt;Aniket Tomar&lt;/strong&gt; on successfully defending his Master’s thesis!&lt;/p&gt;

&lt;p&gt;Aniket’s thesis is entitled &lt;em&gt;Exploring Correspondences Between Gibsonian and Telic Affordances for Object Grasping using 3D Geometry&lt;/em&gt;. This is a really interesting work in that Aniket took what was initially a negative result and probed it enough to draw a robust conclusion about the representation of Gibsonian and telic affordaces in 3D data and the difficulty of embodied tasks for even specialized machine learning models. Part of this work appeared at our &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/AREA-2022.pdf&quot;&gt;AREA workshop paper&lt;/a&gt; at ESSLLI last summer.&lt;/p&gt;

&lt;p&gt;Congratulations to Aniket!  It’s been a pleasure working with you and I look forward to seeing what you do next!&lt;/p&gt;

&lt;p&gt;Here we are after the defense with Aniket’s external committee: myself, Prof. Nathaniel Blanchard and Prof. Ben Clegg from Psychology.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/spring23/aniket-defense.jpg?raw=true&quot; alt=&quot;Aniket Tomar MS Thesis Defense&quot; title=&quot;Aniket Tomar MS Thesis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2023/03/06/congratulations-aniket-tomar.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Mon, 06 Mar 2023 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//congratulations-aniket-tomar</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//congratulations-aniket-tomar</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>New Paper at ACS</title>
        <description>&lt;p&gt;SIGNAL Lab Ph.D. student Sadaf Ghaffari presented &lt;em&gt;Detecting and Accommodating Novel Types and Concepts in an Embodied Simulation Environment&lt;/em&gt; at the Annual Conference on Advances in Cognitive Systems at George Mason University in Arlington, VA.  In this paper, we explore techniques to imbue neural networks with the capability to expand to accommodate new concepts and to detect when the neural model itself is inadequate to the environment, relying on information extracted from an embodied simulation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ghaffari, S. and Krishnaswamy, N. (2022). &lt;em&gt;Detecting and Accommodating Novel Types and Concepts in an Embodied Simulation Environment.&lt;/em&gt; In &lt;em&gt;Annual Conference on Advances in Cognitive Systems (ACS)&lt;/em&gt;. Cognitive Systems Foundation.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 21 Nov 2022 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//new-paper-at-acs-2022</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//new-paper-at-acs-2022</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>Congrats to Fall '22 M.S. Project Presenters</title>
        <description>&lt;p&gt;Congratulations to M.S. students Ramya Sree Patchava and Kush Pandya who presented their M.S. Project posters at the CSU CS Graduate Research Symposium yesterday!&lt;/p&gt;

&lt;p&gt;Very proud to see these two bring their research to a successful conclusion and graduate!&lt;/p&gt;

&lt;p&gt;Ramya’s project: &lt;strong&gt;Evaluating Interchangeability of Face Feature Vectors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kush’s project: &lt;strong&gt;Qualitative Spatial Relation Representation with ML Embedding Spaces&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here we are at the conclusion of yesterday’s event:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/fall22/posters-ramya-kush.jpg?raw=true&quot; alt=&quot;Ramya and Kush's Posters&quot; title=&quot;Ramya and Kush's Posters&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Oct 2022 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//fall-22-ms-project-posters</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//fall-22-ms-project-posters</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>Congratulations to Abhijnan Nath!</title>
        <description>&lt;p&gt;Congratulations to &lt;strong&gt;Abhijnan Nath&lt;/strong&gt; for defending his Master’s thesis!&lt;/p&gt;

&lt;p&gt;Abhijnan’s thesis is entitled &lt;em&gt;Linear Mappings: Semantic Transfer from Transformer Models for Cognate Detection and Coreference Resolution&lt;/em&gt;. Abhijnan argues that high-dimensional vector semantic information can be transfered between Transformer-based language models using a simple affine transformation technique that draws upon previous insights from vision researchers from CSU. He presents evidence from two distinct linguistic tasks: bilingual cognate detection and cross-document coreference eresolution. Part of this research appears in our VarDial workship paper from COLING this year and part of it was funded by the DARPA AIDA program.  Abhijnan will be continuing in the SIGNAL Lab for a Ph.D. starting in Fall 2022!&lt;/p&gt;

&lt;p&gt;Congratulations to Abhijnan!  It’s been a pleasure to work with you and I look forward to continuing our collaboration!&lt;/p&gt;

&lt;p&gt;Here we are after the defense with Abhijnan’s external committee member, Prof. Emily King from Mathematics.  Not shown: Prof. Nate Blanchard, who was out of town and attended remotely.  Getting a student to photoshop him in.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/fall22/abhijnan-defense.jpg?raw=true&quot; alt=&quot;Abhijnan Nath MS Thesis Defense&quot; title=&quot;Abhijnan Nath MS Thesis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2022/10/21/congratulations-abhijnan-nath.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Oct 2022 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//congratulations-abhijnan-nath</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//congratulations-abhijnan-nath</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>Summer/Fall Publications</title>
        <description>&lt;p&gt;It’s been a busy summer!&lt;/p&gt;

&lt;p&gt;In addition to the publications mentioned previously (&lt;em&gt;The VoxWorld Platform for Multimodal Embodied Agents&lt;/em&gt;, &lt;em&gt;Exploiting Embodied Simulation to Detect Novel Object Classes Through Interaction&lt;/em&gt;, and &lt;em&gt;A deep dive into microphones for recording collaborative group work&lt;/em&gt;), members of the SIGNAL Lab were involved with a slew of other papers that were presented during the summer or will be in the fall:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Challenges and Opportunities in Annotating a Multimodal Collaborative Problem Solving Task&lt;/em&gt; and &lt;em&gt;Multimodal Features for Group Dynamic-Aware Agents&lt;/em&gt; were presented by SIGNAL Lab Ph.D. student Mariah Bradford at the Workshop on Interdisciplinary Approaches to Getting AI Experts and Education Stakeholders Talking (Bridging AIEd) at AIEd in Durham, UK, in late July. These papers address questions in feature extraction and annotation in small group tasks for the NSF AI Institute for Student-AI Teaming (iSAT).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;iSAT speech-based AI display for small group collaboration in classrooms&lt;/em&gt; won the Best Interactive Event Award at AIEd! SIGNAL Lab Ph.D. student Ibrahim Khebour worked on this demo for iSAT.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Exploring Correspondences Between Gibsonian and Telic Affordances for Object Grasping&lt;/em&gt; was presented at the 2nd Workshop on Annotation, Recognition and Evaluation of Actions (AREA) at ESSLLI in Galway, Ireland in early August. This paper is adapted from part of Aniket Tomar’s Master’s thesis work on grasping affordances.&lt;/p&gt;

&lt;p&gt;Ph.D. student Sheikh Mannan’s paper &lt;em&gt;Where am I and where should I go? Grounding positional and directional labels in a disoriented human balancing task&lt;/em&gt; applies AI and language models to a novel task involving a sophisiticated notion of embodiment.  It will be presented at the Conference on (Dis)embodiment, hosted by the Centre for Linguistic Theory and Studies in Probability (CLASP) at the University of Gothenburg in Sweden on September 16.&lt;/p&gt;

&lt;p&gt;Finally, &lt;em&gt;A Generalized Method for Automated Multilingual Loanword Detection&lt;/em&gt; will appear as a poster at COLING 2022 in Gyeongju, Korea in October. This paper was a true team effort by researchers Abhijnan Nath, Sina Mahdipour Saravani, Ibrahim Khebour, Sheikh Mannan, and Zihui Li, in which we present a novel approach to detecting loanwords across arbitrary language pairs. Its companion paper &lt;em&gt;Phonetic, Semantic, and Articulatory Features in Assamese-Bengali Cognate Detection&lt;/em&gt;, led by Abhijnan, will be presented at the associated Workshop on NLP for Similar Languages, Varieties, and Dialects (VarDial).&lt;/p&gt;

&lt;p&gt;Congratulations to all these students, and many others who have work under review or in preparation!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Bradford, M., Hansen, P., Lai, K., Brutti, R., Dickler, R., Hirshfield, L. M., Pustejovsky, J., Blanchard, N., and Krishnaswamy, N. (2022). &lt;em&gt;Challenges and Opportunities in Annotating a Multimodal Collaborative Problem Solving Task.&lt;/em&gt; In &lt;em&gt;Workshop on Interdisciplinary Approaches to Getting AI Experts and Education Stakeholders Talking (Bridging AIEd)&lt;/em&gt;. International AIEd Society.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Castillon, I., Venkatesha, V., VanderHoeven, H., Bradford, M., Krishnaswamy, N., and Blanchard, N. (2022). &lt;em&gt;Multimodal Features for Group Dynamic-Aware Agents.&lt;/em&gt; In &lt;em&gt;Workshop on Interdisciplinary Approaches to Getting AI Experts and Education Stakeholders Talking (Bridging AIEd)&lt;/em&gt;. International AIEd Society.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dickler, R., Foltz, P. W., Krishnaswamy, N., Whitehill, J., Weatherly, J., Bodzianowski, M., Perkoff, M., Southwell, R., Pugh, S., Bush, J., Chang, M., Hirshfield, L. M., Showers, D., Ganesh, A., Li, Z., Danilyuk, E., He, X., Khebour, I., Dey, I., Puntambekar, S., and D’Mello, S. K. (2022). &lt;em&gt;iSAT speech-based AI display for small group collaboration in classrooms&lt;/em&gt;. In Interactive event at International Conference on Artificial Intelligence in Education (AIEd). International AIEd Society.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tomar, A. and Krishnaswamy, N. (2022). &lt;em&gt;Exploring Correspondences Between Gibsonian and Telic Affordances for Object Grasping.&lt;/em&gt; In &lt;em&gt;Workshop on Annotation, Recognition and Evaluation of Actions (AREA)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mannan, S. and Krishnaswamy, N. (2022). &lt;em&gt;Where am I and where should I go? Grounding positional and directional labels in a disoriented human balancing task.&lt;/em&gt; &lt;em&gt;In Conference on (Dis)embodiment&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nath, A., Mahdipour Saravani, S., Khebour, I., Mannan, S., Li, Z., and Krishnaswamy, N. (2022). &lt;em&gt;A Generalized Method for Automated Multilingual Loanword Detection.&lt;/em&gt; In &lt;em&gt;International Conference on Computational Linguistics (COLING)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nath, A., Ghosh, R., and Krishnaswamy, N. (2022). &lt;em&gt;Phonetic, Semantic, and Articulatory Features in Assamese-Bengali Cognate Detection.&lt;/em&gt; In &lt;em&gt;Workshop on NLP for Similar Languages, Varieties, and Dialects (VarDial)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 08 Sep 2022 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//summer-fall-publications</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//summer-fall-publications</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>3 New Publications</title>
        <description>&lt;p&gt;Three upcoming publications:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The VoxWorld Platform for Multimodal Embodied Agents&lt;/em&gt; will be a poster and demo, and published in the proceedings of the Language Resources and Evaluation Conference (LREC) in Marseille, France, in June.  This publication is a 5-year retrospective on the VoxWorld platform and presents 3 of the agents developed with it.  This paper was written with multiple collaborators including future SIGNAL Lab Ph.D. student Brittany Cates.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Krishnaswamy, N., Pickard, W., Cates, B., Blanchard, N., and Pustejovsky, J. (2022). &lt;em&gt;The VoxWorld Platform for Multimodal Embodied Agents&lt;/em&gt;. In &lt;em&gt;Language Resources and Evaluation Conference&lt;/em&gt; (LREC).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Exploiting Embodied Simulation to Detect Novel Object Classes Through Interaction&lt;/em&gt; will be a poster at the Annual Meeting of the Cognitive Science Society in Toronto, ON, Canada, in July.  This publication presents early zero-shot transfer learning work based on object interactions with SIGNAL Lab Ph.D. student Sadaf Ghaffari.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Krishnaswamy, N., and Ghaffari, S. (2022). &lt;em&gt;Exploiting Embodied Simulation to Detect Novel Object Classes Through Interaction&lt;/em&gt;. In &lt;em&gt;Annual Meeting of the Cognitive Science Society&lt;/em&gt; (CogSci).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;A deep dive into microphones for recording collaborative group work&lt;/em&gt; will be a poster and short paper at the International Conference on Educational Data Mining (EDM) in Durham, England, in July.  In this paper we examine the effects of various audio hardware stacks on downstream educational data mining tasks such as speech recognition.  This paper was written with many collaborators at CSU including future SIGNAL Lab Ph.D. student Mariah Bradford.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bradford, M., Hansen, P., Beveridge, R., Krishnaswamy, N., and Blanchard, N. (2022). &lt;em&gt;A deep dive into microphones for recording collaborative group work&lt;/em&gt;. In &lt;em&gt;International Conference on Educational Data Mining&lt;/em&gt; (EDM).&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 17 Apr 2022 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//3-new-publications</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//3-new-publications</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>Congratulations to Dhruva Patil and Jason Garcia!</title>
        <description>&lt;p&gt;Congratulations to Dr. &lt;strong&gt;Dhruva Patil&lt;/strong&gt; for defending his Ph.D. dissertation and to &lt;strong&gt;Jason Garcia&lt;/strong&gt; for defending his Master’s thesis!&lt;/p&gt;

&lt;p&gt;Dhruva’s dissertation is entitled &lt;em&gt;Something Is Fishy! How ambiguous language affects generalization of video action recognition networks&lt;/em&gt;. In this research, he analyses weaknesses in the label set of the popular Something-Something video action recognition dataset, and discusses how the one-hot encoding of class labels in computer vision tasks may effectively be hamstringing deep neural networks in vision tasks.  Dhruva will be starting a research position at Amazon in June.&lt;/p&gt;

&lt;p&gt;Jason’s thesis is entitled &lt;em&gt;Applications of Topological Data Analysis to Natural Language Processing and Computer Vision&lt;/em&gt;. Jason applies techniques of topological data analysis, a mathematical technique for exploring the “shape” of data, to problems in NLP and computer vision, including a novel analysis of the topological properties of the encoder layers in BERT.  Jason will be continuing at CSU for a Ph.D. in Math.&lt;/p&gt;

&lt;p&gt;Congratulations to Dhruva and Jason!  It’s been a pleasure to work with you and to see you acheive your degrees!&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2022/04/11/congratulations-dhruva-patil-jason-garcia.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Apr 2022 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//congratulations-dhruva-patil-jason-garcia</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//congratulations-dhruva-patil-jason-garcia</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>Congrats to M.S. Project Presenters</title>
        <description>&lt;p&gt;Congratulations to M.S. students Shivani Mogullapalli, Shriram Gaddam, and Mohit Kumar Katragadda who presented their M.S. Project posters at the CSU CS Graduate Research Symposium yesterday!&lt;/p&gt;

&lt;p&gt;CSU Computer Science has a non-thesis research initiation option for master’s students wherein the student must conduct a research project under the guidance of a faculty member and present a poster on their project in their last semester.  Very proud of the work these three have done this semester and I can wait to see them wrap up the work and graduate!&lt;/p&gt;

&lt;p&gt;Mohit’s project: &lt;strong&gt;Distributed Training of 3D Object Recognition Using Point Clouds&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Shivani’s project: &lt;strong&gt;Mapping Between Face Recogniton Feature Vector from Various CNN Models&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Shriram’s project: &lt;strong&gt;Exploring Embedding Spaces in Transformers by Mapping Feature Vectors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Some pics from yesterday’s event are below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/fall21/poster-shriram.jpg?raw=true&quot; alt=&quot;Exploring Embedding Spaces in Transformers by Mapping Feature Vectors&quot; title=&quot;Exploring Embedding Spaces in Transformers by Mapping Feature Vectors&quot; /&gt;&lt;img src=&quot;../assets/images/fall21/poster-mohit.jpg?raw=true&quot; alt=&quot;Distributed Training of 3D Object Recognition Using Point Clouds&quot; title=&quot;Distributed Training of 3D Object Recognition Using Point Clouds&quot; /&gt;&lt;img src=&quot;../assets/images/fall21/poster-shivani.jpg?raw=true&quot; alt=&quot;Mapping Between Face Recogniton Feature Vector from Various CNN Models&quot; title=&quot;Mapping Between Face Recogniton Feature Vector from Various CNN Models&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 30 Oct 2021 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//ms-project-posters</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//ms-project-posters</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>New Publication at HCI+NLP Workshop @ EACL</title>
        <description>&lt;p&gt;SIGNAL Lab Ph.D. student Nada Alalyani and I have a new publication that will be appearing at the &lt;a href=&quot;https://sites.google.com/view/hciandnlp/home?authuser=0&quot;&gt;Bridging HCI and NLP&lt;/a&gt; workshop at the EACL conference in April!  &lt;em&gt;Embodied Multimodal Agents to Bridge the Understanding Gap&lt;/em&gt; argues that embodied multimodal agents such as virtual avatars and robots play an important role in moving natural language processing toward “deep understanding,” and a demonstrable retrieval of communicative intent from an utterance, a la the definition propounded by Emily Bender and Alexander Koller in their “&lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.463.pdf&quot;&gt;Climbing towards NLU&lt;/a&gt;” paper from last year’s ACL.  We discuss the role of multimodal conversational systems in computationally modeling contextually-grounded communication, and discuss ongoing experiments in multimodal referring expression interpretation and generation as an illustrative use case, using CSU/Brandeis/UF’s very own &lt;a href=&quot;http://www.embodiedhci.net&quot;&gt;Diana&lt;/a&gt; agent.&lt;/p&gt;

&lt;p&gt;The HCI+NLP workshop will be held in conjunction with the EACL conference, April 20th.  The conference will be held virtually (may we banish COVID and return to in-person events soon!).  Before the proceedings are released, the paper can be viewed &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/HCI+NLP-2021.pdf&quot;&gt;here&lt;/a&gt; on my personal website, and will be appearing on arXiv soon.&lt;/p&gt;
</description>
        <pubDate>Thu, 18 Mar 2021 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//new-publication-hci-nlp</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//new-publication-hci-nlp</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>Best Demo at ICAT-EGVE 2020</title>
        <description>&lt;p&gt;Our demo, &lt;em&gt;Situational Awareness in Human Computer Interaction: Diana’s World&lt;/em&gt; has won the &lt;a href=&quot;https://icat-egve-2020.org/awards/&quot;&gt;best demo award&lt;/a&gt; at ICAT-EGVE (International Conference on Artificial Reality and Telexistence &amp;amp; Eurographics Symposium on Virtual Environments), an ACM conference!  Our paper is &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/ICAT-EGVE-2020.pdf&quot;&gt;here&lt;/a&gt; and our demo video can be viewed &lt;a href=&quot;https://www.youtube.com/watch?v=0b2_PWS_QZ4&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The demo was presented on December 4th at the virtual conference.&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2020/12/08/best-demo-icat-egve-2020.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Dec 2020 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//best-demo-icat-egve-2020</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//best-demo-icat-egve-2020</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        <category>awards</category>
        
        
      </item>
    
  </channel>
</rss>
