<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SIGNAL Lab</title>
    <description>Situated Grounding and Natural Language &lt;br/&gt; NLP @ CSU</description>
    <link>https://www.signallab.ai//</link>
    <atom:link href="https://www.signallab.ai//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 09 Sep 2022 03:08:07 +0000</pubDate>
    <lastBuildDate>Fri, 09 Sep 2022 03:08:07 +0000</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Summer/Fall Publications</title>
        <description>&lt;p&gt;It’s been a busy summer!&lt;/p&gt;

&lt;p&gt;In addition to the publications mentioned previously (&lt;em&gt;The VoxWorld Platform for Multimodal Embodied Agents&lt;/em&gt;, &lt;em&gt;Exploiting Embodied Simulation to Detect Novel Object Classes Through Interaction&lt;/em&gt;, and &lt;em&gt;A deep dive into microphones for recording collaborative group work&lt;/em&gt;), members of the SIGNAL Lab were involved with a slew of other papers that were presented during the summer or will be in the fall:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Challenges and Opportunities in Annotating a Multimodal Collaborative Problem Solving Task&lt;/em&gt; and &lt;em&gt;Multimodal Features for Group Dynamic-Aware Agents&lt;/em&gt; were presented by SIGNAL Lab Ph.D. student Mariah Bradford at the Workshop on Interdisciplinary Approaches to Getting AI Experts and Education Stakeholders Talking (Bridging AIEd) at AIEd in Durham, UK, in late July. These papers address questions in feature extraction and annotation in small group tasks for the NSF AI Institute for Student-AI Teaming (iSAT).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;iSAT speech-based AI display for small group collaboration in classrooms&lt;/em&gt; won the Best Interactive Event Award at AIEd! SIGNAL Lab Ph.D. student Ibrahim Khebour worked on this demo for iSAT.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Exploring Correspondences Between Gibsonian and Telic Affordances for Object Grasping&lt;/em&gt; was presented at the 2nd Workshop on Annotation, Recognition and Evaluation of Actions (AREA) at ESSLLI in Galway, Ireland in early August. This paper is adapted from part of Aniket Tomar’s Master’s thesis work on grasping affordances.&lt;/p&gt;

&lt;p&gt;Ph.D. student Sheikh Mannan’s paper &lt;em&gt;Where am I and where should I go? Grounding positional and directional labels in a disoriented human balancing task&lt;/em&gt; applies AI and language models to a novel task involving a sophisiticated notion of embodiment.  It will be presented at the Conference on (Dis)embodiment, hosted by the Centre for Linguistic Theory and Studies in Probability (CLASP) at the University of Gothenburg in Sweden on September 16.&lt;/p&gt;

&lt;p&gt;Finally, &lt;em&gt;A Generalized Method for Automated Multilingual Loanword Detection&lt;/em&gt; will appear as a poster at COLING 2022 in Gyeongju, Korea. This paper was a true team effort by researchers Abhijnan Nath, Sina Mahdipour Saravani, Ibrahim Khebour, Sheikh Mannan, and Zihui Li, in which we present a novel approach to detecting loanwords across arbitrary language pairs. Its companion paper &lt;em&gt;Phonetic, Semantic, and Articulatory Features in Assamese-Bengali Cognate Detection&lt;/em&gt;, led by Abhijnan, will be presented at the associated Workshop on NLP for Similar Languages, Varieties, and Dialects (VarDial).&lt;/p&gt;

&lt;p&gt;Congratulations to all these students, and many others who have work under review or in preparation!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Bradford, M., Hansen, P., Lai, K., Brutti, R., Dickler, R., Hirshfield, L. M., Pustejovsky, J., Blanchard, N., and Krishnaswamy, N. (2022). &lt;em&gt;Challenges and Opportunities in Annotating a Multimodal Collaborative Problem Solving Task.&lt;/em&gt; In &lt;em&gt;Workshop on Interdisciplinary Approaches to Getting AI Experts and Education Stakeholders Talking (Bridging AIEd)&lt;/em&gt;. International AIEd Society.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Castillon, I., Venkatesha, V., VanderHoeven, H., Bradford, M., Krishnaswamy, N., and Blanchard, N. (2022). &lt;em&gt;Multimodal Features for Group Dynamic-Aware Agents.&lt;/em&gt; In &lt;em&gt;Workshop on Interdisciplinary Approaches to Getting AI Experts and Education Stakeholders Talking (Bridging AIEd)&lt;/em&gt;. International AIEd Society.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dickler, R., Foltz, P. W., Krishnaswamy, N., Whitehill, J., Weatherly, J., Bodzianowski, M., Perkoff, M., Southwell, R., Pugh, S., Bush, J., Chang, M., Hirshfield, L. M., Showers, D., Ganesh, A., Li, Z., Danilyuk, E., He, X., Khebour, I., Dey, I., Puntambekar, S., and D’Mello, S. K. (2022). &lt;em&gt;iSAT speech-based AI display for small group collaboration in classrooms&lt;/em&gt;. In Interactive event at International Conference on Artificial Intelligence in Education (AIEd). International AIEd Society.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tomar, A. and Krishnaswamy, N. (2022). &lt;em&gt;Exploring Correspondences Between Gibsonian and Telic Affordances for Object Grasping.&lt;/em&gt; In &lt;em&gt;Workshop on Annotation, Recognition and Evaluation of Actions (AREA)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mannan, S. and Krishnaswamy, N. (2022). &lt;em&gt;Where am I and where should I go? Grounding positional and directional labels in a disoriented human balancing task.&lt;/em&gt; &lt;em&gt;In Conference on (Dis)embodiment&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nath, A., Mahdipour Saravani, S., Khebour, I., Mannan, S., Li, Z., and Krishnaswamy, N. (2022). &lt;em&gt;A Generalized Method for Automated Multilingual Loanword Detection.&lt;/em&gt; In &lt;em&gt;International Conference on Computational Linguistics (COLING)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nath, A., Ghosh, R., and Krishnaswamy, N. (2022). &lt;em&gt;Phonetic, Semantic, and Articulatory Features in Assamese-Bengali Cognate Detection.&lt;/em&gt; In &lt;em&gt;Workshop on NLP for Similar Languages, Varieties, and Dialects (VarDial)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 08 Sep 2022 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//summer-fall-publications</link>
        <guid isPermaLink="true">https://www.signallab.ai//summer-fall-publications</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>3 New Publications</title>
        <description>&lt;p&gt;Three upcoming publications:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The VoxWorld Platform for Multimodal Embodied Agents&lt;/em&gt; will be a poster and demo, and published in the proceedings of the Language Resources and Evaluation Conference (LREC) in Marseille, France, in June.  This publication is a 5-year retrospective on the VoxWorld platform and presents 3 of the agents developed with it.  This paper was written with multiple collaborators including future SIGNAL Lab Ph.D. student Brittany Cates.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Krishnaswamy, N., Pickard, W., Cates, B., Blanchard, N., and Pustejovsky, J. (2022). &lt;em&gt;The VoxWorld Platform for Multimodal Embodied Agents&lt;/em&gt;. In &lt;em&gt;Language Resources and Evaluation Conference&lt;/em&gt; (LREC).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Exploiting Embodied Simulation to Detect Novel Object Classes Through Interaction&lt;/em&gt; will be a poster at the Annual Meeting of the Cognitive Science Society in Toronto, ON, Canada, in July.  This publication presents early zero-shot transfer learning work based on object interactions with SIGNAL Lab Ph.D. student Sadaf Ghaffari.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Krishnaswamy, N., and Ghaffari, S. (2022). &lt;em&gt;Exploiting Embodied Simulation to Detect Novel Object Classes Through Interaction&lt;/em&gt;. In &lt;em&gt;Annual Meeting of the Cognitive Science Society&lt;/em&gt; (CogSci).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;A deep dive into microphones for recording collaborative group work&lt;/em&gt; will be a poster and short paper at the International Conference on Educational Data Mining (EDM) in Durham, England, in July.  In this paper we examine the effects of various audio hardware stacks on downstream educational data mining tasks such as speech recognition.  This paper was written with many collaborators at CSU including future SIGNAL Lab Ph.D. student Mariah Bradford.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bradford, M., Hansen, P., Beveridge, R., Krishnaswamy, N., and Blanchard, N. (2022). &lt;em&gt;A deep dive into microphones for recording collaborative group work&lt;/em&gt;. In &lt;em&gt;International Conference on Educational Data Mining&lt;/em&gt; (EDM).&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 17 Apr 2022 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//3-new-publications</link>
        <guid isPermaLink="true">https://www.signallab.ai//3-new-publications</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>Congratulations to Dhruva Patil and Jason Garcia!</title>
        <description>&lt;p&gt;Congratulations to Dr. &lt;strong&gt;Dhruva Patil&lt;/strong&gt; for defending his Ph.D. dissertation and to &lt;strong&gt;Jason Garcia&lt;/strong&gt; for defending his Master’s thesis!&lt;/p&gt;

&lt;p&gt;Dhruva’s dissertation is entitled &lt;em&gt;Something Is Fishy! How ambiguous language affects generalization of video action recognition networks&lt;/em&gt;. In this research, he analyses weaknesses in the label set of the popular Something-Something video action recognition dataset, and discusses how the one-hot encoding of class labels in computer vision tasks may effectively be hamstringing deep neural networks in vision tasks.  Dhruva will be starting a research position at Amazon in June.&lt;/p&gt;

&lt;p&gt;Jason’s thesis is entitled &lt;em&gt;Applications of Topological Data Analysis to Natural Language Processing and Computer Vision&lt;/em&gt;. Jason applies techniques of topological data analysis, a mathematical technique for exploring the “shape” of data, to problems in NLP and computer vision, including a novel analysis of the topological properties of the encoder layers in BERT.  Jason will be continuing at CSU for a Ph.D. in Math.&lt;/p&gt;

&lt;p&gt;Congratulations to Dhruva and Jason!  It’s been a pleasure to work with you and to see you acheive your degrees!&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2022/04/11/congratulations-dhruva-patil-jason-garcia.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Apr 2022 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//congratulations-dhruva-patil-jason-garcia</link>
        <guid isPermaLink="true">https://www.signallab.ai//congratulations-dhruva-patil-jason-garcia</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>Congrats to M.S. Project Presenters</title>
        <description>&lt;p&gt;Congratulations to M.S. students Shivani Mogullapalli, Shriram Gaddam, and Mohit Kumar Katragadda who presented their M.S. Project posters at the CSU CS Graduate Research Symposium yesterday!&lt;/p&gt;

&lt;p&gt;CSU Computer Science has a non-thesis research initiation option for master’s students wherein the student must conduct a research project under the guidance of a faculty member and present a poster on their project in their last semester.  Very proud of the work these three have done this semester and I can wait to see them wrap up the work and graduate!&lt;/p&gt;

&lt;p&gt;Mohit’s project: &lt;strong&gt;Distributed Training of 3D Object Recognition Using Point Clouds&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Shivani’s project: &lt;strong&gt;Mapping Between Face Recogniton Feature Vector from Various CNN Models&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Shriram’s project: &lt;strong&gt;Exploring Embedding Spaces in Transformers by Mapping Feature Vectors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Some pics from yesterday’s event are below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/fall21/poster-shriram.jpg?raw=true&quot; alt=&quot;Exploring Embedding Spaces in Transformers by Mapping Feature Vectors&quot; title=&quot;Exploring Embedding Spaces in Transformers by Mapping Feature Vectors&quot; /&gt;&lt;img src=&quot;../assets/images/fall21/poster-mohit.jpg?raw=true&quot; alt=&quot;Distributed Training of 3D Object Recognition Using Point Clouds&quot; title=&quot;Distributed Training of 3D Object Recognition Using Point Clouds&quot; /&gt;&lt;img src=&quot;../assets/images/fall21/poster-shivani.jpg?raw=true&quot; alt=&quot;Mapping Between Face Recogniton Feature Vector from Various CNN Models&quot; title=&quot;Mapping Between Face Recogniton Feature Vector from Various CNN Models&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 30 Oct 2021 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//ms-project-posters</link>
        <guid isPermaLink="true">https://www.signallab.ai//ms-project-posters</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>New Publication at HCI+NLP Workshop @ EACL</title>
        <description>&lt;p&gt;SIGNAL Lab Ph.D. student Nada Alalyani and I have a new publication that will be appearing at the &lt;a href=&quot;https://sites.google.com/view/hciandnlp/home?authuser=0&quot;&gt;Bridging HCI and NLP&lt;/a&gt; workshop at the EACL conference in April!  &lt;em&gt;Embodied Multimodal Agents to Bridge the Understanding Gap&lt;/em&gt; argues that embodied multimodal agents such as virtual avatars and robots play an important role in moving natural language processing toward “deep understanding,” and a demonstrable retrieval of communicative intent from an utterance, a la the definition propounded by Emily Bender and Alexander Koller in their “&lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.463.pdf&quot;&gt;Climbing towards NLU&lt;/a&gt;” paper from last year’s ACL.  We discuss the role of multimodal conversational systems in computationally modeling contextually-grounded communication, and discuss ongoing experiments in multimodal referring expression interpretation and generation as an illustrative use case, using CSU/Brandeis/UF’s very own &lt;a href=&quot;http://www.embodiedhci.net&quot;&gt;Diana&lt;/a&gt; agent.&lt;/p&gt;

&lt;p&gt;The HCI+NLP workshop will be held in conjunction with the EACL conference, April 20th.  The conference will be held virtually (may we banish COVID and return to in-person events soon!).  Before the proceedings are released, the paper can be viewed &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/HCI+NLP-2021.pdf&quot;&gt;here&lt;/a&gt; on my personal website, and will be appearing on arXiv soon.&lt;/p&gt;
</description>
        <pubDate>Thu, 18 Mar 2021 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//new-publication-hci-nlp</link>
        <guid isPermaLink="true">https://www.signallab.ai//new-publication-hci-nlp</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>Best Demo at ICAT-EGVE 2020</title>
        <description>&lt;p&gt;Our demo, &lt;em&gt;Situational Awareness in Human Computer Interaction: Diana’s World&lt;/em&gt; has won the &lt;a href=&quot;https://icat-egve-2020.org/awards/&quot;&gt;best demo award&lt;/a&gt; at ICAT-EGVE (International Conference on Artificial Reality and Telexistence &amp;amp; Eurographics Symposium on Virtual Environments), an ACM conference!  Our paper is &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/ICAT-EGVE-2020.pdf&quot;&gt;here&lt;/a&gt; and our demo video can be viewed &lt;a href=&quot;https://www.youtube.com/watch?v=0b2_PWS_QZ4&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The demo was presented on December 4th at the virtual conference.&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2020/12/08/best-demo-icat-egve-2020.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Dec 2020 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//best-demo-icat-egve-2020</link>
        <guid isPermaLink="true">https://www.signallab.ai//best-demo-icat-egve-2020</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        <category>awards</category>
        
        
      </item>
    
      <item>
        <title>SIGNAL Lab Colloquium Video</title>
        <description>&lt;p&gt;Traditionally, computer science faculty at CSU give rapid-fire research presentations at the beginning of the fall semester to present themselves to students and discuss research opportunities.  Due to the Covid situation pushing everything online, the seminar format is being flipped this year, with prerecorded videos being offered followed by Q+A during the scheduled session time.  I took the opportunity to create a 5-minute video overview of some of the SIGNAL Lab’s major research thrusts, complete with some video demonstrations.&lt;/p&gt;

&lt;p&gt;You can watch the video &lt;a href=&quot;http://nkrishna.powweb.com/video/BMAC-short.mp4&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Thu, 27 Aug 2020 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//signal-lab-colloquium-video</link>
        <guid isPermaLink="true">https://www.signallab.ai//signal-lab-colloquium-video</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        
      </item>
    
      <item>
        <title>Neurosymbolic AI at ACS 2020</title>
        <description>&lt;p&gt;Today I presented a long paper, “Neurosymbolic AI for Situated Language Understanding” at the Advances in Cognitive Systems conference, held virtually and hosted by the Palo Alto Research Center.&lt;/p&gt;

&lt;p&gt;This was third of three papers submitted as a postdoc and presented as a professor, and I’m really very proud of this one, as it’s a detailed yet concise summary of effectively the last five years of work at Brandeis in developing situated grounding and embodied AI under the Communicating With Computers program, and nicely lays out most of my graduate and postdoctoral career.&lt;/p&gt;

&lt;p&gt;The work discussed in this paper forms the foundations of the work in the SIGNAL Lab, and I hope that neurosymbolic AI is just getting started, and provides a number of opportunities for groundbreaking research that are barely on the horizon for the AI and cognitive systems communities.  You can find the paper &lt;a href=&quot;assets/docs/pdfs/ACS-2020.pdf&quot;&gt;here&lt;/a&gt;, the slides &lt;a href=&quot;assets/docs/slides/ACS-2020.pdf&quot;&gt;here&lt;/a&gt;, and a video of the talk &lt;a href=&quot;https://www.youtube.com/watch?v=IwIvn64mT3U&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2020/08/10/neurosymbolic-ai-at-acs-2020.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Mon, 10 Aug 2020 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//neurosymbolic-ai-at-acs-2020</link>
        <guid isPermaLink="true">https://www.signallab.ai//neurosymbolic-ai-at-acs-2020</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>Introducing SIGNAL Lab</title>
        <description>&lt;p&gt;I’m pleased to announce the formation of my lab here at Colorado State, the &lt;strong&gt;Si&lt;/strong&gt;tuated &lt;strong&gt;G&lt;/strong&gt;rounding and &lt;strong&gt;Na&lt;/strong&gt;tural &lt;strong&gt;L&lt;/strong&gt;anguage Lab.  As the primary NLP group in the CSU Computer Science department, we aim to produce cutting-edge AI research in language processing and understanding that’s grounded in linguistic knowledge and multimodality, as well as cognitive science, philosophy, psychology, and more.  We collaborate extensively with other CSU CS labs, including the &lt;a href=&quot;https://www.cs.colostate.edu/~draper/CwC.php&quot;&gt;CWC&lt;/a&gt; and &lt;a href=&quot;https://nuilab.org/Home&quot;&gt;NUI&lt;/a&gt; Labs, and with groups at Brandeis University, Tufts University, and others.  Broadly, we study language and cognition using machine learning, logical architectures, and simulation methods. Much of our work involves intelligent, interactive, multimodal agents.&lt;/p&gt;

&lt;p&gt;We are currently seeking new researchers and collaborators, including students interested in language, AI, and machine learning. Prior experience or interest in graphics or software engineering is a plus, and women and underrepresented minorities are particularly welcome. Interested in working with us? Please &lt;a href=&quot;contact/&quot;&gt;get in touch!&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 01 Jul 2020 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//introducing-signal-lab</link>
        <guid isPermaLink="true">https://www.signallab.ai//introducing-signal-lab</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        
      </item>
    
  </channel>
</rss>
