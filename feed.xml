<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SIGNAL Lab</title>
    <description>Situated Grounding and Natural Language &lt;br/&gt; NLP @ CSU</description>
    <link>https://www.signallab.ai/https://www.signallab.ai//</link>
    <atom:link href="https://www.signallab.ai/https://www.signallab.ai//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 25 Feb 2024 03:21:11 +0000</pubDate>
    <lastBuildDate>Sun, 25 Feb 2024 03:21:11 +0000</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>DARPA FACT AIE Award</title>
        <description>&lt;p&gt;I am beyond excited that we have been awarded a contract on the DARPA Friction and Accountability in Conversational Transactions (FACT) AI Exploration program! Our project, &lt;em&gt;TRACE: Transparency, Reflection, and Accountability in Conversational Exchanges&lt;/em&gt;, will address a lack of “friction” (deliberation and reflective reasoning) in LLMs that prevent them from being used reliably in mission-critical workflows.&lt;/p&gt;

&lt;p&gt;Put simply, LLMs are incentivized to accept the premises in whatever is input to them, even if false or faulty, and generate responses accordingly. This is due to an inability to track and assess the validity of beliefs held by their interlocutors. Theory of Mind (ToM) provides a way of measuring and tracking such beliefs, such as in the course of a shared collaborative task. Therefore we’re going to build ToM into LLMs.&lt;/p&gt;

&lt;p&gt;This project is a collaboration with Dr. James Pustejovsky at Brandeis University. Other CSU personnel include Dr. Nate Blanchard, Dr. Sarath Sreedharan, and Dr. Bruce Draper. I’m very excited to lead this excellent team!&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2024/02/23/darpa-fact-aie-award.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Fri, 23 Feb 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//darpa-fact-aie-award</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//darpa-fact-aie-award</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grants</category>
        
        
      </item>
    
      <item>
        <title>LREC-COLING Hat Trick and Other Stories</title>
        <description>&lt;p&gt;We are delighted to be 3 for 3 on submissions to LREC-COLING 2024, the first Joint International Conference on Computational Linguistics and Language Resources and Evaluation! The accepted papers are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Nath, A., Jamil, H., Ahmed, S. R., Baker, G., Ghosh, R., Martin, J. H., Blanchard, N., and Krishnaswamy, N. (2024). Multimodal Cross-Document Event Coreference Resolution Using Linear Semantic Transfer and Mixed-Modality Ensembles. In &lt;em&gt;Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING).&lt;/em&gt; ACL.
(In collaboration with the University of Colorado Boulder.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Khebour, I., Lai, K., Bradford, M., Zhu, Y., Brutti, R., Tam, C., Tu, J., Ibarra, B., Blanchard, N., Krishnaswamy, N., and Pustejovsky, J. (2024). Common Ground Tracking in Multimodal Dialogue. In &lt;em&gt;Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING).&lt;/em&gt; ACL.
(In collaboration with Brandeis University.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Manafi, S. and Krishnaswamy, N. (2024). Cross-Lingual Transfer Robustness to Lower-Resource Languages on Adversarial Datasets. In &lt;em&gt;Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING).&lt;/em&gt; ACL.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The conference will be held in Turin, Italy, in late May. Congratulations to SIGNAL Lab graduate students Abhijnan, Ibrahim, Mariah, and Shadi (and all affiliated students/external collaborators)!&lt;/p&gt;

&lt;p&gt;Additionally, Sheikh Mannan and Sadaf Ghaffari have a publication each at AAAI Spring Symposia, to be held at Stanford University in March:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ghaffari, S. and Krishnaswamy, N. (2024). Exploring Failure Cases in Multimodal Reasoning About Physical Dynamics. In &lt;em&gt;AAAI Spring Symposium: Empowering Machine Learning and Large Language Models with Domain and Commonsense Knowledge (MAKE).&lt;/em&gt; AAAI.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mannan, S., Vimal, V. P., DiZio, P., and Krishnaswamy, N. (2024). Embodying Human-Like Modes of Balance Control Through Human-in-the-Loop Dyadic Learning. In &lt;em&gt;AAAI Spring Symposium: Symposium on Human-Like Learning (HLL).&lt;/em&gt; AAAI.
(In collaboration with Brandeis University.)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Last but not least, another hat trick (+1 bonus poster) at the International Conference on Human-Computer Interaction, to be held in Washington, DC, in June/July:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;VanderHoeven, H., Bradford, M., Jung, C., Khebour, I., Lai, K., Pustejovsky, J., Krishnaswamy, N., and Blanchard, N. (2024). Multimodal Design for Interactive Collaborative Problem Solving Support. In &lt;em&gt;International Conference on Human-Computer Interaction (HCII).&lt;/em&gt; Springer.
(In collaboration with Brandeis University.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Zhu, Y., VanderHoeven, H., Lai, K., Bradford, M., Tam, C., Khebour, I., Brutti, R., Krishnaswamy, N., and Pustejovsky, J. (2024). Modeling Theory of Mind in Multimodal HCI. In &lt;em&gt;International Conference on Human-Computer Interaction (HCII).&lt;/em&gt; Springer.
(In collaboration with Brandeis University.)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;VanderHoeven, H., Blanchard, N., and Krishnaswamy, N. (2024). Point Target Detection for Multimodal Communication. In &lt;em&gt;International Conference on Human-Computer Interaction (HCII).&lt;/em&gt; Springer.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Congratulations to Hannah (x3), Mariah, and Ibrahim again!&lt;/p&gt;

&lt;p&gt;And the poster:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Seefried, E., Bradford, M., Aich, S., Siebert, C., Krishnaswamy, N., and Blanchard, N. (2024). Learning Foreign Language Vocabulary Through Task-Based Virtual Reality Immersion. In &lt;em&gt;International Conference on Human-Computer Interaction (HCII).&lt;/em&gt; Springer.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 20 Feb 2024 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//coling-hat-trick-other-stories</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//coling-hat-trick-other-stories</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>RETTL Grant Awarded</title>
        <description>&lt;p&gt;Pleased to announce that our grant proposal to NSF’s RETTL program, &lt;em&gt;An AI Tutoring System for Pollinator Conservation Community Science Training&lt;/em&gt;, has been awarded, for $849,890!&lt;/p&gt;

&lt;p&gt;This project is led by my colleague Dr. Sarath Sreedharan and includes Dr. Nate Blanchard of CSU’s Vision Lab and Dr. Jill Zarestky in the CSU School of Education.  In this project we will develop AI-powered tools for citizen science, focusing on identification of different pollinator species.  Let’s get them bees!&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2023/09/11/rettl-grant-awarded.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Mon, 11 Sep 2023 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//rettl-grant-awarded</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//rettl-grant-awarded</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grants</category>
        
        
      </item>
    
      <item>
        <title>Spring 2023 Publication Round Up</title>
        <description>&lt;p&gt;At the end of the semester, time to take stock of lab activity:&lt;/p&gt;

&lt;p&gt;Hannah VanderHoeven’s paper &lt;em&gt;Robust Motion Recognition using Gesture Phase Annotation&lt;/em&gt; was accepted for publication at HCII 2023 in Copenhagen, Denmark in July!&lt;/p&gt;

&lt;p&gt;Also at HCII will be &lt;em&gt;Intentional Microgesture Recognition for Extended Human-Computer Interaction&lt;/em&gt;, with Sheikh Mannan and Hannah as contributing authors!&lt;/p&gt;

&lt;p&gt;Mariah Bradford and Ibrahim Khebour’s paper &lt;em&gt;Automatic Detection of Collaborative States in Small Groups Using Multimodal Features&lt;/em&gt; was accepted for publication at AIEd 2023 in Tokyo, Japan, also in July!&lt;/p&gt;

&lt;p&gt;Mariah and Ibrahim also contributed to &lt;em&gt;How Good is Automatic Segmentation as a Multimodal Discourse Annotation Aid?&lt;/em&gt;, accepted for presentation at the Interoperable Semantic Annotation workshop at IWCS in June.&lt;/p&gt;

&lt;p&gt;Sadaf Ghaffari’s paper &lt;em&gt;Grounding and Distinguishing Conceptual Vocabulary Through Similarity Learning in Embodied Simulations&lt;/em&gt; was accepted for publication at the IWCS 2023 main conference in Nancy, France!&lt;/p&gt;

&lt;p&gt;And finally, Abhijnan Nath and Mannan’s paper &lt;em&gt;AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese&lt;/em&gt; was accepted for publication in Findings of ACL 2023, the companion volume to the ACL 2023 conference!  Abhijnan was also second author on &lt;em&gt;2*n is better than n^2: Decomposing Event Coreference Resolution into Two Tractable Problems&lt;/em&gt;, a collaboration with members of the BoulderNLP group, also accepted to Finding of ACL 2023.  ACL 2023 will be held in Toronto, ON, Canada, in July.&lt;/p&gt;

&lt;p&gt;Look out for these papers, and more to come soon.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;VanderHoeven, H., Blanchard, N., and Krishnaswamy, N. (2023). &lt;em&gt;Robust Motion Recognition using Gesture Phase Annotation.&lt;/em&gt; In &lt;em&gt;International Conference on Human-Computer Interaction (HCII)&lt;/em&gt;. Springer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Kandoi, C., Jung, C., Mannan, S., VanderHoeven, H., Meisman, Q., Krishnaswamy, N., and Blanchard, N. (2023). &lt;em&gt;Intentional Microgesture Recognition for Extended Human-Computer Interaction.&lt;/em&gt; In &lt;em&gt;International Conference on Human-Computer Interaction (HCII)&lt;/em&gt;. Springer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bradford, M., Khebour, I., Blanchard, N., and Krishnaswamy, N. (2023). &lt;em&gt;Automatic Detection of Collaborative States in Small Groups Using Multimodal Features.&lt;/em&gt; In &lt;em&gt;International Conference on Artificial Intelligence in Education (AIEd)&lt;/em&gt;. International AIEd Society.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Terpstra, C., Khebour, I., Bradford, M., Wisniewski, B., Krishnaswamy, N., and Blanchard, N. (2023). &lt;em&gt;How Good is Automatic Segmentation as a Multimodal Discourse Annotation Aid?&lt;/em&gt; In &lt;em&gt;International Workshop on Semantic Annotation (ISA)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ghaffari, S. and Krishnaswamy, N. (2023). &lt;em&gt;Grounding and Distinguishing Conceptual Vocabulary Through Similarity Learning in Embodied Simulations.&lt;/em&gt; In &lt;em&gt;International Conference on Computational Semantics (IWCS)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nath, A., Mannan, S., and Krishnaswamy, N. (2023). &lt;em&gt;AxomiyaBERTa: A Phonologically-aware Transformer Model for Assamese.&lt;/em&gt; In &lt;em&gt;Findings of the Association for Computational Linguistics (Findings of ACL)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ahmed, S. R., Nath, A., Martin, J. H., and Krishnaswamy, N. (2023). &lt;em&gt;2*n is better than n^2: Decomposing Event Coreference Resolution into Two Tractable Problems.&lt;/em&gt; In &lt;em&gt;Findings of the Association for Computational Linguistics (Findings of ACL)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 14 May 2023 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//spring-2023-publication-roundup</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//spring-2023-publication-roundup</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>Congratulations to Aniket Tomar!</title>
        <description>&lt;p&gt;Congratulations to &lt;strong&gt;Aniket Tomar&lt;/strong&gt; on successfully defending his Master’s thesis!&lt;/p&gt;

&lt;p&gt;Aniket’s thesis is entitled &lt;em&gt;Exploring Correspondences Between Gibsonian and Telic Affordances for Object Grasping using 3D Geometry&lt;/em&gt;. This is a really interesting work in that Aniket took what was initially a negative result and probed it enough to draw a robust conclusion about the representation of Gibsonian and telic affordaces in 3D data and the difficulty of embodied tasks for even specialized machine learning models. Part of this work appeared in our &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/AREA-2022.pdf&quot;&gt;AREA workshop paper&lt;/a&gt; at ESSLLI last summer.&lt;/p&gt;

&lt;p&gt;Congratulations to Aniket!  It’s been a pleasure working with you and I look forward to seeing what you do next!&lt;/p&gt;

&lt;p&gt;Here we are after the defense with Aniket’s committee: myself, Prof. Nathaniel Blanchard and Prof. Ben Clegg from Psychology.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/spring23/aniket-defense.jpg?raw=true&quot; alt=&quot;Aniket Tomar MS Thesis Defense&quot; title=&quot;Aniket Tomar MS Thesis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2023/03/06/congratulations-aniket-tomar.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Mon, 06 Mar 2023 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//congratulations-aniket-tomar</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//congratulations-aniket-tomar</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>New Paper at ACS</title>
        <description>&lt;p&gt;SIGNAL Lab Ph.D. student Sadaf Ghaffari presented &lt;em&gt;Detecting and Accommodating Novel Types and Concepts in an Embodied Simulation Environment&lt;/em&gt; at the Annual Conference on Advances in Cognitive Systems at George Mason University in Arlington, VA.  In this paper, we explore techniques to imbue neural networks with the capability to expand to accommodate new concepts and to detect when the neural model itself is inadequate to the environment, relying on information extracted from an embodied simulation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ghaffari, S. and Krishnaswamy, N. (2022). &lt;em&gt;Detecting and Accommodating Novel Types and Concepts in an Embodied Simulation Environment.&lt;/em&gt; In &lt;em&gt;Annual Conference on Advances in Cognitive Systems (ACS)&lt;/em&gt;. Cognitive Systems Foundation.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Mon, 21 Nov 2022 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//new-paper-at-acs-2022</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//new-paper-at-acs-2022</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>Congrats to Fall '22 M.S. Project Presenters</title>
        <description>&lt;p&gt;Congratulations to M.S. students Ramya Sree Patchava and Kush Pandya who presented their M.S. Project posters at the CSU CS Graduate Research Symposium yesterday!&lt;/p&gt;

&lt;p&gt;Very proud to see these two bring their research to a successful conclusion and graduate!&lt;/p&gt;

&lt;p&gt;Ramya’s project: &lt;strong&gt;Evaluating Interchangeability of Face Feature Vectors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Kush’s project: &lt;strong&gt;Qualitative Spatial Relation Representation with ML Embedding Spaces&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here we are at the conclusion of yesterday’s event:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/fall22/posters-ramya-kush.jpg?raw=true&quot; alt=&quot;Ramya and Kush's Posters&quot; title=&quot;Ramya and Kush's Posters&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Oct 2022 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//fall-22-ms-project-posters</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//fall-22-ms-project-posters</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>Congratulations to Abhijnan Nath!</title>
        <description>&lt;p&gt;Congratulations to &lt;strong&gt;Abhijnan Nath&lt;/strong&gt; for defending his Master’s thesis!&lt;/p&gt;

&lt;p&gt;Abhijnan’s thesis is entitled &lt;em&gt;Linear Mappings: Semantic Transfer from Transformer Models for Cognate Detection and Coreference Resolution&lt;/em&gt;. Abhijnan argues that high-dimensional vector semantic information can be transfered between Transformer-based language models using a simple affine transformation technique that draws upon previous insights from vision researchers from CSU. He presents evidence from two distinct linguistic tasks: bilingual cognate detection and cross-document coreference eresolution. Part of this research appears in our VarDial workship paper from COLING this year and part of it was funded by the DARPA AIDA program.  Abhijnan will be continuing in the SIGNAL Lab for a Ph.D. starting in Fall 2022!&lt;/p&gt;

&lt;p&gt;Congratulations to Abhijnan!  It’s been a pleasure to work with you and I look forward to continuing our collaboration!&lt;/p&gt;

&lt;p&gt;Here we are after the defense with Abhijnan’s external committee member, Prof. Emily King from Mathematics.  Not shown: Prof. Nate Blanchard, who was out of town and attended remotely.  Getting a student to photoshop him in.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/fall22/abhijnan-defense.jpg?raw=true&quot; alt=&quot;Abhijnan Nath MS Thesis Defense&quot; title=&quot;Abhijnan Nath MS Thesis&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2022/10/21/congratulations-abhijnan-nath.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Fri, 21 Oct 2022 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//congratulations-abhijnan-nath</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//congratulations-abhijnan-nath</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>grads</category>
        
        
      </item>
    
      <item>
        <title>Summer/Fall Publications</title>
        <description>&lt;p&gt;It’s been a busy summer!&lt;/p&gt;

&lt;p&gt;In addition to the publications mentioned previously (&lt;em&gt;The VoxWorld Platform for Multimodal Embodied Agents&lt;/em&gt;, &lt;em&gt;Exploiting Embodied Simulation to Detect Novel Object Classes Through Interaction&lt;/em&gt;, and &lt;em&gt;A deep dive into microphones for recording collaborative group work&lt;/em&gt;), members of the SIGNAL Lab were involved with a slew of other papers that were presented during the summer or will be in the fall:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Challenges and Opportunities in Annotating a Multimodal Collaborative Problem Solving Task&lt;/em&gt; and &lt;em&gt;Multimodal Features for Group Dynamic-Aware Agents&lt;/em&gt; were presented by SIGNAL Lab Ph.D. student Mariah Bradford at the Workshop on Interdisciplinary Approaches to Getting AI Experts and Education Stakeholders Talking (Bridging AIEd) at AIEd in Durham, UK, in late July. These papers address questions in feature extraction and annotation in small group tasks for the NSF AI Institute for Student-AI Teaming (iSAT).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;iSAT speech-based AI display for small group collaboration in classrooms&lt;/em&gt; won the Best Interactive Event Award at AIEd! SIGNAL Lab Ph.D. student Ibrahim Khebour worked on this demo for iSAT.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Exploring Correspondences Between Gibsonian and Telic Affordances for Object Grasping&lt;/em&gt; was presented at the 2nd Workshop on Annotation, Recognition and Evaluation of Actions (AREA) at ESSLLI in Galway, Ireland in early August. This paper is adapted from part of Aniket Tomar’s Master’s thesis work on grasping affordances.&lt;/p&gt;

&lt;p&gt;Ph.D. student Sheikh Mannan’s paper &lt;em&gt;Where am I and where should I go? Grounding positional and directional labels in a disoriented human balancing task&lt;/em&gt; applies AI and language models to a novel task involving a sophisiticated notion of embodiment.  It will be presented at the Conference on (Dis)embodiment, hosted by the Centre for Linguistic Theory and Studies in Probability (CLASP) at the University of Gothenburg in Sweden on September 16.&lt;/p&gt;

&lt;p&gt;Finally, &lt;em&gt;A Generalized Method for Automated Multilingual Loanword Detection&lt;/em&gt; will appear as a poster at COLING 2022 in Gyeongju, Korea in October. This paper was a true team effort by researchers Abhijnan Nath, Sina Mahdipour Saravani, Ibrahim Khebour, Sheikh Mannan, and Zihui Li, in which we present a novel approach to detecting loanwords across arbitrary language pairs. Its companion paper &lt;em&gt;Phonetic, Semantic, and Articulatory Features in Assamese-Bengali Cognate Detection&lt;/em&gt;, led by Abhijnan, will be presented at the associated Workshop on NLP for Similar Languages, Varieties, and Dialects (VarDial).&lt;/p&gt;

&lt;p&gt;Congratulations to all these students, and many others who have work under review or in preparation!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Citations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Bradford, M., Hansen, P., Lai, K., Brutti, R., Dickler, R., Hirshfield, L. M., Pustejovsky, J., Blanchard, N., and Krishnaswamy, N. (2022). &lt;em&gt;Challenges and Opportunities in Annotating a Multimodal Collaborative Problem Solving Task.&lt;/em&gt; In &lt;em&gt;Workshop on Interdisciplinary Approaches to Getting AI Experts and Education Stakeholders Talking (Bridging AIEd)&lt;/em&gt;. International AIEd Society.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Castillon, I., Venkatesha, V., VanderHoeven, H., Bradford, M., Krishnaswamy, N., and Blanchard, N. (2022). &lt;em&gt;Multimodal Features for Group Dynamic-Aware Agents.&lt;/em&gt; In &lt;em&gt;Workshop on Interdisciplinary Approaches to Getting AI Experts and Education Stakeholders Talking (Bridging AIEd)&lt;/em&gt;. International AIEd Society.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dickler, R., Foltz, P. W., Krishnaswamy, N., Whitehill, J., Weatherly, J., Bodzianowski, M., Perkoff, M., Southwell, R., Pugh, S., Bush, J., Chang, M., Hirshfield, L. M., Showers, D., Ganesh, A., Li, Z., Danilyuk, E., He, X., Khebour, I., Dey, I., Puntambekar, S., and D’Mello, S. K. (2022). &lt;em&gt;iSAT speech-based AI display for small group collaboration in classrooms&lt;/em&gt;. In Interactive event at International Conference on Artificial Intelligence in Education (AIEd). International AIEd Society.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Tomar, A. and Krishnaswamy, N. (2022). &lt;em&gt;Exploring Correspondences Between Gibsonian and Telic Affordances for Object Grasping.&lt;/em&gt; In &lt;em&gt;Workshop on Annotation, Recognition and Evaluation of Actions (AREA)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Mannan, S. and Krishnaswamy, N. (2022). &lt;em&gt;Where am I and where should I go? Grounding positional and directional labels in a disoriented human balancing task.&lt;/em&gt; &lt;em&gt;In Conference on (Dis)embodiment&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nath, A., Mahdipour Saravani, S., Khebour, I., Mannan, S., Li, Z., and Krishnaswamy, N. (2022). &lt;em&gt;A Generalized Method for Automated Multilingual Loanword Detection.&lt;/em&gt; In &lt;em&gt;International Conference on Computational Linguistics (COLING)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nath, A., Ghosh, R., and Krishnaswamy, N. (2022). &lt;em&gt;Phonetic, Semantic, and Articulatory Features in Assamese-Bengali Cognate Detection.&lt;/em&gt; In &lt;em&gt;Workshop on NLP for Similar Languages, Varieties, and Dialects (VarDial)&lt;/em&gt;. ACL.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 08 Sep 2022 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//summer-fall-publications</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//summer-fall-publications</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
      <item>
        <title>3 New Publications</title>
        <description>&lt;p&gt;Three upcoming publications:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The VoxWorld Platform for Multimodal Embodied Agents&lt;/em&gt; will be a poster and demo, and published in the proceedings of the Language Resources and Evaluation Conference (LREC) in Marseille, France, in June.  This publication is a 5-year retrospective on the VoxWorld platform and presents 3 of the agents developed with it.  This paper was written with multiple collaborators including future SIGNAL Lab Ph.D. student Brittany Cates.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Krishnaswamy, N., Pickard, W., Cates, B., Blanchard, N., and Pustejovsky, J. (2022). &lt;em&gt;The VoxWorld Platform for Multimodal Embodied Agents&lt;/em&gt;. In &lt;em&gt;Language Resources and Evaluation Conference&lt;/em&gt; (LREC).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Exploiting Embodied Simulation to Detect Novel Object Classes Through Interaction&lt;/em&gt; will be a poster at the Annual Meeting of the Cognitive Science Society in Toronto, ON, Canada, in July.  This publication presents early zero-shot transfer learning work based on object interactions with SIGNAL Lab Ph.D. student Sadaf Ghaffari.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Krishnaswamy, N., and Ghaffari, S. (2022). &lt;em&gt;Exploiting Embodied Simulation to Detect Novel Object Classes Through Interaction&lt;/em&gt;. In &lt;em&gt;Annual Meeting of the Cognitive Science Society&lt;/em&gt; (CogSci).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;A deep dive into microphones for recording collaborative group work&lt;/em&gt; will be a poster and short paper at the International Conference on Educational Data Mining (EDM) in Durham, England, in July.  In this paper we examine the effects of various audio hardware stacks on downstream educational data mining tasks such as speech recognition.  This paper was written with many collaborators at CSU including future SIGNAL Lab Ph.D. student Mariah Bradford.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bradford, M., Hansen, P., Beveridge, R., Krishnaswamy, N., and Blanchard, N. (2022). &lt;em&gt;A deep dive into microphones for recording collaborative group work&lt;/em&gt;. In &lt;em&gt;International Conference on Educational Data Mining&lt;/em&gt; (EDM).&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 17 Apr 2022 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai/https://www.signallab.ai//3-new-publications</link>
        <guid isPermaLink="true">https://www.signallab.ai/https://www.signallab.ai//3-new-publications</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        <category>pubs</category>
        
        
      </item>
    
  </channel>
</rss>
