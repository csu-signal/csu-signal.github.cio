<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SIGNAL Lab</title>
    <description>Situated Grounding and Natural Language &lt;br/&gt; NLP @ CSU</description>
    <link>https://www.signallab.ai//</link>
    <atom:link href="https://www.signallab.ai//feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 31 Oct 2021 04:17:48 +0000</pubDate>
    <lastBuildDate>Sun, 31 Oct 2021 04:17:48 +0000</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Congrats to M.S. Project Presenters</title>
        <description>&lt;p&gt;Congratulations to M.S. students Shivani Mogullapalli, Shriram Gaddam, and Mohit Kumar Katragadda who presented their M.S. Project posters at the CSU CS Graduate Research Symposium yesterday!&lt;/p&gt;

&lt;p&gt;CSU Computer Science has a non-thesis research initiation option for master’s students wherein the student must conduct a research project under the guidance of a faculty member and present a poster on their project in their last semester.  Very proud of the work these three have done this semester and I can wait to see them wrap up the work and graduate!&lt;/p&gt;

&lt;p&gt;Mohit’s project: &lt;strong&gt;Distributed Training of 3D Object Recognition Using Point Clouds&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Shivani’s project: &lt;strong&gt;Mapping Between Face Recogniton Feature Vector from Various CNN Models&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Shriram’s project: &lt;strong&gt;Exploring Embedding Spaces in Transformers by Mapping Feature Vectors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Some pics from yesterday’s event are below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/fall21/poster-shriram.jpg?raw=true&quot; alt=&quot;Exploring Embedding Spaces in Transformers by Mapping Feature Vectors&quot; title=&quot;Exploring Embedding Spaces in Transformers by Mapping Feature Vectors&quot; /&gt;&lt;img src=&quot;../assets/images/fall21/poster-mohit.jpg?raw=true&quot; alt=&quot;Distributed Training of 3D Object Recognition Using Point Clouds&quot; title=&quot;Distributed Training of 3D Object Recognition Using Point Clouds&quot; /&gt;&lt;img src=&quot;../assets/images/fall21/poster-shivani.jpg?raw=true&quot; alt=&quot;Mapping Between Face Recogniton Feature Vector from Various CNN Models&quot; title=&quot;Mapping Between Face Recogniton Feature Vector from Various CNN Models&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sat, 30 Oct 2021 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//ms-project-posters</link>
        <guid isPermaLink="true">https://www.signallab.ai//ms-project-posters</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        
      </item>
    
      <item>
        <title>New Publication at HCI+NLP Workshop @ EACL</title>
        <description>&lt;p&gt;SIGNAL Lab Ph.D. student Nada Alalyani and I have a new publication that will be appearing at the &lt;a href=&quot;https://sites.google.com/view/hciandnlp/home?authuser=0&quot;&gt;Bridging HCI and NLP&lt;/a&gt; workshop at the EACL conference in April!  &lt;em&gt;Embodied Multimodal Agents to Bridge the Understanding Gap&lt;/em&gt; argues that embodied multimodal agents such as virtual avatars and robots play an important role in moving natural language processing toward “deep understanding,” and a demonstrable retrieval of communicative intent from an utterance, a la the definition propounded by Emily Bender and Alexander Koller in their “&lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.463.pdf&quot;&gt;Climbing towards NLU&lt;/a&gt;” paper from last year’s ACL.  We discuss the role of multimodal conversational systems in computationally modeling contextually-grounded communication, and discuss ongoing experiments in multimodal referring expression interpretation and generation as an illustrative use case, using CSU/Brandeis/UF’s very own &lt;a href=&quot;http://www.embodiedhci.net&quot;&gt;Diana&lt;/a&gt; agent.&lt;/p&gt;

&lt;p&gt;The HCI+NLP workshop will be held in conjunction with the EACL conference, April 20th.  The conference will be held virtually (may we banish COVID and return to in-person events soon!).  Before the proceedings are released, the paper can be viewed &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/HCI+NLP-2021.pdf&quot;&gt;here&lt;/a&gt; on my personal website, and will be appearing on arXiv soon.&lt;/p&gt;
</description>
        <pubDate>Thu, 18 Mar 2021 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//new-publication-hci-nlp</link>
        <guid isPermaLink="true">https://www.signallab.ai//new-publication-hci-nlp</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        
      </item>
    
      <item>
        <title>Best Demo at ICAT-EGVE 2020</title>
        <description>&lt;p&gt;Our demo, &lt;em&gt;Situational Awareness in Human Computer Interaction: Diana’s World&lt;/em&gt; has won the &lt;a href=&quot;https://icat-egve-2020.org/awards/&quot;&gt;best demo award&lt;/a&gt; at ICAT-EGVE (International Conference on Artificial Reality and Telexistence &amp;amp; Eurographics Symposium on Virtual Environments), an ACM conference!  Our paper is &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/assets/docs/pdfs/ICAT-EGVE-2020.pdf&quot;&gt;here&lt;/a&gt; and our demo video can be viewed &lt;a href=&quot;https://www.youtube.com/watch?v=0b2_PWS_QZ4&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The demo was presented on December 4th at the virtual conference.&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2020/12/08/best-demo-icat-egve-2020.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Dec 2020 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//best-demo-icat-egve-2020</link>
        <guid isPermaLink="true">https://www.signallab.ai//best-demo-icat-egve-2020</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        
      </item>
    
      <item>
        <title>SIGNAL Lab Colloquium Video</title>
        <description>&lt;p&gt;Traditionally, computer science faculty at CSU give rapid-fire research presentations at the beginning of the fall semester to present themselves to students and discuss research opportunities.  Due to the Covid situation pushing everything online, the seminar format is being flipped this year, with prerecorded videos being offered followed by Q+A during the scheduled session time.  I took the opportunity to create a 5-minute video overview of some of the SIGNAL Lab’s major research thrusts, complete with some video demonstrations.&lt;/p&gt;

&lt;p&gt;You can watch the video &lt;a href=&quot;http://nkrishna.powweb.com/video/BMAC-short.mp4&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Thu, 27 Aug 2020 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//signal-lab-colloquium-video</link>
        <guid isPermaLink="true">https://www.signallab.ai//signal-lab-colloquium-video</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        
      </item>
    
      <item>
        <title>Neurosymbolic AI at ACS 2020</title>
        <description>&lt;p&gt;Today I presented a long paper, “Neurosymbolic AI for Situated Language Understanding” at the Advances in Cognitive Systems conference, held virtually and hosted by the Palo Alto Research Center.&lt;/p&gt;

&lt;p&gt;This was third of three papers submitted as a postdoc and presented as a professor, and I’m really very proud of this one, as it’s a detailed yet concise summary of effectively the last five years of work at Brandeis in developing situated grounding and embodied AI under the Communicating With Computers program, and nicely lays out most of my graduate and postdoctoral career.&lt;/p&gt;

&lt;p&gt;The work discussed in this paper forms the foundations of the work in the SIGNAL Lab, and I hope that neurosymbolic AI is just getting started, and provides a number of opportunities for groundbreaking research that are barely on the horizon for the AI and cognitive systems communities.  You can find the paper &lt;a href=&quot;assets/docs/pdfs/ACS-2020.pdf&quot;&gt;here&lt;/a&gt;, the slides &lt;a href=&quot;assets/docs/slides/ACS-2020.pdf&quot;&gt;here&lt;/a&gt;, and a video of the talk &lt;a href=&quot;https://www.youtube.com/watch?v=IwIvn64mT3U&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;(X-posted on &lt;a href=&quot;https://www.nikhilkrishnaswamy.com/2020/08/10/neurosymbolic-ai-at-acs-2020.html&quot;&gt;nikhilkrishnaswamy.com&lt;/a&gt;)&lt;/p&gt;
</description>
        <pubDate>Mon, 10 Aug 2020 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//neurosymbolic-ai-at-acs-2020</link>
        <guid isPermaLink="true">https://www.signallab.ai//neurosymbolic-ai-at-acs-2020</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        
      </item>
    
      <item>
        <title>Introducing SIGNAL Lab</title>
        <description>&lt;p&gt;I’m pleased to announce the formation of my lab here at Colorado State, the &lt;strong&gt;Si&lt;/strong&gt;tuated &lt;strong&gt;G&lt;/strong&gt;rounding and &lt;strong&gt;Na&lt;/strong&gt;tural &lt;strong&gt;L&lt;/strong&gt;anguage Lab.  As the primary NLP group in the CSU Computer Science department, we aim to produce cutting-edge AI research in language processing and understanding that’s grounded in linguistic knowledge and multimodality, as well as cognitive science, philosophy, psychology, and more.  We collaborate extensively with other CSU CS labs, including the &lt;a href=&quot;https://www.cs.colostate.edu/~draper/CwC.php&quot;&gt;CWC&lt;/a&gt; and &lt;a href=&quot;https://nuilab.org/Home&quot;&gt;NUI&lt;/a&gt; Labs, and with groups at Brandeis University, Tufts University, and others.  Broadly, we study language and cognition using machine learning, logical architectures, and simulation methods. Much of our work involves intelligent, interactive, multimodal agents.&lt;/p&gt;

&lt;p&gt;We are currently seeking new researchers and collaborators, including students interested in language, AI, and machine learning. Prior experience or interest in graphics or software engineering is a plus, and women and underrepresented minorities are particularly welcome. Interested in working with us? Please &lt;a href=&quot;contact/&quot;&gt;get in touch!&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 01 Jul 2020 07:00:00 +0000</pubDate>
        <link>https://www.signallab.ai//introducing-signal-lab</link>
        <guid isPermaLink="true">https://www.signallab.ai//introducing-signal-lab</guid>
        
        <category>lab</category>
        
        <category>news</category>
        
        
      </item>
    
  </channel>
</rss>
